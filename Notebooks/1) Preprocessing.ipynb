{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d81876",
   "metadata": {},
   "source": [
    "# News-Headlines-Dataset-For-Sarcasm-Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f6077",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b63298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "\n",
    "# URLS validation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# get_articles \n",
    "from newspaper import Article\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from autocorrect import spell\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "import re \n",
    "import nltk \n",
    "import spacy\n",
    "import string\n",
    "import itertools\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a74e2e",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea493b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file is not empty\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/advancing...</td>\n",
       "      <td>advancing the world's women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/how-meat-...</td>\n",
       "      <td>the fascinating case for eating lab-grown meat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/boxed-col...</td>\n",
       "      <td>this ceo will send your kids to school, if you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://politics.theonion.com/top-snake-handle...</td>\n",
       "      <td>top snake handler leaves sinking huckabee camp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/fridays-m...</td>\n",
       "      <td>friday's morning email: inside trump's presser...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/airline-p...</td>\n",
       "      <td>airline passengers tackle man who rushes cockp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/facebook-...</td>\n",
       "      <td>facebook reportedly working on healthcare feat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.thegu...</td>\n",
       "      <td>north korea praises trump and urges us voters ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jeffrey-l...</td>\n",
       "      <td>actually, cnn's jeffrey lord has been 'indefen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/barcelona...</td>\n",
       "      <td>barcelona holds huge protest in support of ref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://entertainment.theonion.com/nuclear-bom...</td>\n",
       "      <td>nuclear bomb detonates during rehearsal for 's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.theonion.com/cosby-lawyer-asks-why...</td>\n",
       "      <td>cosby lawyer asks why accusers didn't come for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.theonion.com/stock-analysts-confus...</td>\n",
       "      <td>stock analysts confused, frightened by boar ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/bloomberg...</td>\n",
       "      <td>bloomberg's program to build better cities jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/craig-hic...</td>\n",
       "      <td>craig hicks indicted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_link  \\\n",
       "0   https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1   https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2   https://local.theonion.com/mom-starting-to-fea...   \n",
       "3   https://politics.theonion.com/boehner-just-wan...   \n",
       "4   https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "5   https://www.huffingtonpost.com/entry/advancing...   \n",
       "6   https://www.huffingtonpost.com/entry/how-meat-...   \n",
       "7   https://www.huffingtonpost.com/entry/boxed-col...   \n",
       "8   https://politics.theonion.com/top-snake-handle...   \n",
       "9   https://www.huffingtonpost.com/entry/fridays-m...   \n",
       "10  https://www.huffingtonpost.com/entry/airline-p...   \n",
       "11  https://www.huffingtonpost.com/entry/facebook-...   \n",
       "12  https://www.huffingtonpost.comhttp://www.thegu...   \n",
       "13  https://www.huffingtonpost.com/entry/jeffrey-l...   \n",
       "14  https://www.huffingtonpost.com/entry/barcelona...   \n",
       "15  https://entertainment.theonion.com/nuclear-bom...   \n",
       "16  https://www.theonion.com/cosby-lawyer-asks-why...   \n",
       "17  https://www.theonion.com/stock-analysts-confus...   \n",
       "18  https://www.huffingtonpost.com/entry/bloomberg...   \n",
       "19  https://www.huffingtonpost.com/entry/craig-hic...   \n",
       "\n",
       "                                             headline  is_sarcastic  \n",
       "0   former versace store clerk sues over secret 'b...             0  \n",
       "1   the 'roseanne' revival catches up to our thorn...             0  \n",
       "2   mom starting to fear son's web series closest ...             1  \n",
       "3   boehner just wants wife to listen, not come up...             1  \n",
       "4   j.k. rowling wishes snape happy birthday in th...             0  \n",
       "5                         advancing the world's women             0  \n",
       "6      the fascinating case for eating lab-grown meat             0  \n",
       "7   this ceo will send your kids to school, if you...             0  \n",
       "8   top snake handler leaves sinking huckabee camp...             1  \n",
       "9   friday's morning email: inside trump's presser...             0  \n",
       "10  airline passengers tackle man who rushes cockp...             0  \n",
       "11  facebook reportedly working on healthcare feat...             0  \n",
       "12  north korea praises trump and urges us voters ...             0  \n",
       "13  actually, cnn's jeffrey lord has been 'indefen...             0  \n",
       "14  barcelona holds huge protest in support of ref...             0  \n",
       "15  nuclear bomb detonates during rehearsal for 's...             1  \n",
       "16  cosby lawyer asks why accusers didn't come for...             1  \n",
       "17  stock analysts confused, frightened by boar ma...             1  \n",
       "18  bloomberg's program to build better cities jus...             0  \n",
       "19                               craig hicks indicted             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset from csv file\n",
    "def read_file(FILE_PATH):\n",
    "    df = pd.read_json(FILE_PATH)\n",
    "    if(df.empty):\n",
    "        print ('CSV file is empty')\n",
    "    else:\n",
    "        print ('CSV file is not empty')\n",
    "        return df\n",
    "\n",
    "    \n",
    "df = read_file(\"../Data/sarcasm.json\")\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895ca4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('article_link', dtype('O'))\n",
      "('headline', dtype('O'))\n",
      "('is_sarcastic', dtype('int64'))\n"
     ]
    }
   ],
   "source": [
    "for dtype in df.dtypes.iteritems():\n",
    "    print(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27e8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, article_link    https://www.huffingtonpost.com/entry/versace-b...\n",
      "headline        former versace store clerk sues over secret 'b...\n",
      "is_sarcastic                                                    0\n",
      "Name: 0, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "for row in df.head(1).iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edf3be",
   "metadata": {},
   "source": [
    "###### URLS validation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed29a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url(url):\n",
    "    \n",
    "    content = validators.url(url)\n",
    "    \n",
    "    if content is True:\n",
    "            pass\n",
    "    else:\n",
    "        return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ebb4b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      3\u001b[0m     url \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_link\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mis_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mis_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_url\u001b[39m(url):\n\u001b[1;32m----> 3\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mvalidators\u001b[49m\u001b[38;5;241m.\u001b[39murl(url)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validators' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for row in df.iterrows():\n",
    "    url = row[1]['article_link']\n",
    "    result = is_url(url)\n",
    "    if result is None:\n",
    "        pass\n",
    "    else:\n",
    "        results.append(str(result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e05930",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = []\n",
    "\n",
    "for result in results:\n",
    "    indexs.append(df.index[df['article_link'] == result].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb03ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexs:\n",
    "    df['article_link'] = df['article_link'].drop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4eb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_link'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75378ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eef851",
   "metadata": {},
   "source": [
    "###  Articles Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "4318e5d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_Article(url):\n",
    "    try:\n",
    "        # download and parse article\n",
    "        article = Article(url)\n",
    "\n",
    "        article.download()\n",
    "\n",
    "        article.parse()\n",
    "\n",
    "        # Return text\n",
    "        text = article.text\n",
    "\n",
    "        return text\n",
    "    \n",
    "    except:\n",
    "        print(np.nan)\n",
    "df['article'] = df['article_link'].apply(get_Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "77d3f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['article_link'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "85c94d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'../Data/cleaned_articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ff59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a2a60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>A photo posted by Versace (@versace_official) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>The “Roseanne” that existed two decades ago di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE PLAINS, NY—With still no indication that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>WASHINGTON—Amid the continuing debate over the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Professor Snape needs a moment to decide how h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic  \\\n",
       "0  former versace store clerk sues over secret 'b...             0   \n",
       "1  the 'roseanne' revival catches up to our thorn...             0   \n",
       "2  mom starting to fear son's web series closest ...             1   \n",
       "3  boehner just wants wife to listen, not come up...             1   \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0   \n",
       "\n",
       "                                             article  \n",
       "0  A photo posted by Versace (@versace_official) ...  \n",
       "1  The “Roseanne” that existed two decades ago di...  \n",
       "2  WHITE PLAINS, NY—With still no indication that...  \n",
       "3  WASHINGTON—Amid the continuing debate over the...  \n",
       "4  Professor Snape needs a moment to decide how h...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(r'../Data/cleaned_articles.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2399fc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26125, 3)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ebfa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline          0\n",
       "is_sarcastic      0\n",
       "article         107\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda51c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline        0\n",
       "is_sarcastic    0\n",
       "article         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbdfc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26018, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209a783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all category\n",
    "category = list(df['is_sarcastic'].unique())\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ef7e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243ee742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5a2ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine headline and article together\n",
    "\n",
    "df['Article'] = df['headline'].str.cat(df['article'], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75503671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['headline', 'article'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6915cbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                            Article\n",
       "0             0  former versace store clerk sues over secret 'b...\n",
       "1             0  the 'roseanne' revival catches up to our thorn...\n",
       "2             1  mom starting to fear son's web series closest ...\n",
       "3             1  boehner just wants wife to listen, not come up...\n",
       "4             0  j.k. rowling wishes snape happy birthday in th..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4408dd",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Normally texts have:\n",
    "<ol>\n",
    "<li>too many spelling mistakes in the text</li>\n",
    "<li>too many numbers and punctuations</li>\n",
    "<li>too many emojis and emoticons and username and links too. </li>\n",
    "<li>Some of the text parts are not in the English language. Data is having a mixture of more than one language</li>\n",
    "<li>Some of the words are combined with the hyphen or data having contractions word or Repetitions of words.</li>\n",
    "\n",
    "</ol>\n",
    "\n",
    "<p>Here i will clean the text by doing the following steps:\n",
    "<ol>\n",
    "\n",
    "<li>Lowecasing the data</li>\n",
    "<li>Removing Puncuatations</li>\n",
    "<li>Removing Numbers</li>\n",
    "<li>Removing extra space</li>\n",
    "<li>Removing Contractions</li>\n",
    "<li>Removing HTML tags</li>\n",
    "<li>Removing & Finding URL and Email id</li>\n",
    "<li>Removing Stop Words</li>\n",
    "<li>Standardizing and Spell Check</li>\n",
    "<li>Remove some Extra-words</li>\n",
    "<li>Stemming</li>\n",
    "<li>Spell Correction</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffb8ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Normalizing Text (lower) \n",
    "\n",
    "def normalize(content):\n",
    "    content = content.lower()\n",
    "    return content\n",
    "\n",
    "# 2) Removing Puncuatations\n",
    "\n",
    "def remove_punct(content):\n",
    "    content = content.translate(content.maketrans(\"\", \"\", string.punctuation))\n",
    "    return content\n",
    "\n",
    "# 3) cleaning digits\n",
    "\n",
    "def remove_num(content):\n",
    "    content = ''.join([i for i in content if not i.isdigit()])\n",
    "\n",
    "    return content\n",
    "\n",
    "# 4) Remove extra-space\n",
    "\n",
    "def remove_spaces(content):\n",
    "    content = \" \".join(content.split())\n",
    "\n",
    "    return content\n",
    "\n",
    "# 5)# Remove Contraction\n",
    "\n",
    "def remove_cont(content):\n",
    "    content = contractions.fix(content)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 6) Remove Html Tags\n",
    "\n",
    "def remove_html(content):\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # return data by retrieving the tag content\n",
    "    content = ' '.join(soup.stripped_strings)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 7) Remove URLs and E-mails (UniCode)\n",
    "\n",
    "def remove_unicode(content):\n",
    "    content = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", content)\n",
    "    return content\n",
    "\n",
    "# 8) Removing Stop Words\n",
    "\n",
    "def remove_stopword(content):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    content = \" \".join([word for word in str(content).split() if word not in STOPWORDS])\n",
    "    return content\n",
    "\n",
    "# 9) Standardizing and Spell Check\n",
    "\n",
    "def Standardize(content):\n",
    "    content = ''.join(''.join(s)[:2] for _, s in itertools.groupby(content))\n",
    "    spell = Speller(lang='en')\n",
    "    content = spell(content)\n",
    "    return content\n",
    "\n",
    "# 10) Remove some Extra-words\n",
    "\n",
    "def remove_extrawords(content):\n",
    "    stop=['href','lt','gt','ii','iii','ie','quot','com']  ## This all words are most repeated words it does not make any sense\n",
    "    content = content.split(\" \")\n",
    "    filtered_list=[]\n",
    "    for i in content:\n",
    "        if i not in stop:\n",
    "            filtered_list.append(i)\n",
    "            \n",
    "    content = ' '.join(filtered_list)\n",
    "    return content\n",
    "\n",
    "\n",
    "# 11) Stemming\n",
    "\n",
    "def Stemming(content):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    content = stemmer.stem(content)\n",
    "    return content\n",
    "\n",
    "\n",
    "# 12) Spell Correction\n",
    "\n",
    "\n",
    "    content = stemmer.stem(spell(content))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47c8dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    \n",
    "    # Normalize\n",
    "    \n",
    "    Normalized_content = normalize(content)\n",
    "    \n",
    "    # Removing Puncuatations\n",
    "    \n",
    "    clean_content = remove_punct(Normalized_content)\n",
    "\n",
    "    # cleaning digits\n",
    "\n",
    "    clean_content = remove_num(clean_content)\n",
    "\n",
    "    # Remove extra-space\n",
    "    \n",
    "    clean_content = remove_spaces(clean_content)\n",
    "    \n",
    "    # Remove Contraction\n",
    "\n",
    "    clean_content = remove_cont(clean_content)\n",
    "    \n",
    "    # Remove Html Tags\n",
    "\n",
    "    clean_content = remove_html(clean_content)\n",
    "    \n",
    "    # Remove URLs and E-mails (UniCode)\n",
    "\n",
    "    clean_content = remove_unicode(clean_content)\n",
    "    \n",
    "    # Removing Stop Words\n",
    "\n",
    "    clean_content = remove_stopword(clean_content)\n",
    "\n",
    "    # Remove some Extra-words\n",
    "    \n",
    "    clean_content = remove_extrawords(clean_content)\n",
    "\n",
    "    return clean_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42478155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Article'] = df['Article'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e3f0c",
   "metadata": {},
   "source": [
    "### 6 ) Saving datasets obtained from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9390dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'../Data/Articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e3b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
