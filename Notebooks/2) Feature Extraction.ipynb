{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d11a44",
   "metadata": {},
   "source": [
    "# Articles Recommendation Categorization\n",
    "\n",
    "Recommending web articles for the learners for different study programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79b41d",
   "metadata": {},
   "source": [
    "### 1) Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe9096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import re \n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BOW, TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "# Encoding the target variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "## Word Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339ed37",
   "metadata": {},
   "source": [
    "### 2) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a09f518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>former versace store clerk sues secret black c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>roseanne revival catches thorny political mood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mom starting fear sons web series closest thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>boehner wants wife listen come alternative deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>jk rowling wishes snape happy birthday magical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                            Article\n",
       "0             0  former versace store clerk sues secret black c...\n",
       "1             0  roseanne revival catches thorny political mood...\n",
       "2             1  mom starting fear sons web series closest thin...\n",
       "3             1  boehner wants wife listen come alternative deb...\n",
       "4             0  jk rowling wishes snape happy birthday magical..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset from csv file\n",
    "df = pd.read_json(r'../Data/Articles.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d6629e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25966, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be599b45",
   "metadata": {},
   "source": [
    "### 3) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16dd80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine data and target\n",
    "X = df['Article']\n",
    "y = df.iloc[:, 0].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c26668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Training dataset:  19474\n",
      "Length of the Test dataset:  6492\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training, validation and Test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "\n",
    "#Data Summary\n",
    "print('Length of the Training dataset: ',len(X_train))\n",
    "print('Length of the Test dataset: ',len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce12c9a",
   "metadata": {},
   "source": [
    "###### Bag of words (Count Vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e489f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (19474,)\n",
      "Shape of X_test: (6492,)\n",
      "Shape of train_vectors: (19474, 132117)\n",
      "Shape of test_vectors: (6492, 132117)\n",
      "Computational time: 12.309767723083496\n"
     ]
    }
   ],
   "source": [
    "## Vectorization of data\n",
    "## Vectorize the data using Bag of words (BOW)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "tf_vec = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stop_words)\n",
    "\n",
    "train_features = tf_vec.fit(X_train)\n",
    "train_features = tf_vec.transform(X_train)\n",
    "test_features = tf_vec.transform(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Shape of X_train:',X_train.shape)\n",
    "print('Shape of X_test:',X_test.shape)\n",
    "print('Shape of train_vectors:',train_features.shape)\n",
    "print('Shape of test_vectors:',test_features.shape)\n",
    "\n",
    "print('Computational time:',end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eca6e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save BOW vectors\n",
    "with open('../vectors/BOW_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../vectors/BOW_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_features, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019a349",
   "metadata": {},
   "source": [
    "###### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a0ed4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (19474,)\n",
      "Shape of X_test: (6492,)\n",
      "Shape of train_vectors: (19474, 2000)\n",
      "Shape of test_vectors: (6492, 2000)\n",
      "Computational time: 33.56597566604614\n"
     ]
    }
   ],
   "source": [
    "# I will use TF-IDF method to extract the text features.\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tf_vec = TfidfVectorizer(tokenizer=None, stop_words='english', max_df=0.75, max_features=2000, lowercase=False,\n",
    "                         ngram_range=(1,2), use_idf=False, sublinear_tf=True, min_df=5, norm='l2',\n",
    "                         encoding='latin-1')\n",
    "\n",
    "\n",
    "train_features = tf_vec.fit(X_train)\n",
    "train_features = tf_vec.transform(X_train)\n",
    "test_features = tf_vec.transform(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print('Shape of X_train:',X_train.shape)\n",
    "print('Shape of X_test:',X_test.shape)\n",
    "print('Shape of train_vectors:',train_features.shape)\n",
    "print('Shape of test_vectors:',test_features.shape)\n",
    "\n",
    "print('Computational time:',end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ab855e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save tfidf vectorizer\n",
    "\n",
    "with open('../vectors/TFIDF_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../vectors/TFIDF_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_features, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cdc7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save Target vectors\n",
    "\n",
    "with open('../vectors/train_label.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../vectors/test_label.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9e325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
